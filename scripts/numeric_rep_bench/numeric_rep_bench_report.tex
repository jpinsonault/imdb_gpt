\documentclass[11pt]{article}

\usepackage[margin=1.1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{titlesec}

\setlength{\parskip}{6pt}
\setlength{\parindent}{0pt}

\graphicspath{{plots/}}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=black,
    citecolor=black,
    pdfauthor={},
    pdftitle={Numeric Representation Benchmark Report},
    pdfborder={0 0 0}
}

\titleformat{\section}{\large\bfseries}{\thesection}{0.75em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{0.5em}{}
\titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection}{0.5em}{}

% --- Run-time macros (fill in from your automation if desired) ---
\newcommand{\RunID}{RUN\_ID}
\newcommand{\TrainRegime}{medium}
\newcommand{\PrimaryMetric}{R$^2$ (regression) / accuracy (classification)}
\newcommand{\repname}[1]{\texttt{\detokenize{#1}}}

\begin{document}

\begin{center}
    {\LARGE Numeric Representation Benchmark}\\[4pt]
    {\large Scalar vs Digit Encodings}\\[6pt]
    {\normalsize Run ID: \RunID}
\end{center}

\vspace{1em}

\section{Overview}

This report summarizes an automated benchmark evaluating how different encodings
of real-valued inputs affect downstream performance on simple supervised tasks.
All configurations share the same model family and training protocol; only the
numeric representation is varied.

The comparison focuses on:
\begin{itemize}[noitemsep,topsep=2pt]
    \item scalar encodings with simple scalings, and
    \item digit-wise encodings (different bases, resolutions, and sign handling).
\end{itemize}

Unless otherwise noted, the default training regime is \texttt{\TrainRegime},
and we report the primary metric as \PrimaryMetric.

\section{Benchmark Design}

\subsection{Tasks}

Each task uses low-dimensional real inputs with deterministic targets:
\begin{itemize}[noitemsep,topsep=2pt]
    \item \texttt{add}: $y = x_0 + x_1$
    \item \texttt{mul}: $y = x_0 \cdot x_1$
    \item \texttt{sin}: $y = \sin(x_0)$
    \item \texttt{gt}: binary label for $(x_0 > x_1)$
    \item \texttt{parity}: parity of an integer-valued input
\end{itemize}

These span linear, nonlinear, periodic, and discrete/bit-like structure.

\subsection{Numeric Regimes}
\label{subsec:regimes}

For all tasks except parity, inputs are drawn from one of several regimes:
\begin{itemize}[noitemsep,topsep=2pt]
    \item \texttt{small}: narrow range around $0$
    \item \texttt{medium}: moderate range around $0$
    \item \texttt{large}: wide range with larger magnitudes
    \item \texttt{mixed}: mixture of small and large scales
    \item \texttt{near\_zero}: very small-magnitude values
\end{itemize}

Unless specified otherwise, models are trained on \texttt{\TrainRegime} and
evaluated across all regimes to probe generalization and robustness.

\subsection{Representations}

Each real input coordinate is encoded using one of two families.

\subsubsection{Scalar encodings}

Scalar encodings map each value to a single float:
\begin{itemize}[noitemsep,topsep=2pt]
    \item \repname{scalar\_none}: raw value
    \item \repname{scalar\_standardize}: z-score normalization
    \item \repname{scalar\_minmax}: min--max scaling
    \item \repname{scalar\_log}: $\log(1+x)$ on appropriate ranges
\end{itemize}

\subsubsection{Digit encodings}

Digit encodings map each value to a sequence of discrete symbols:
\begin{itemize}[noitemsep,topsep=2pt]
    \item base $b \in \{2,4,5,10,16\}$,
    \item a fixed number of integer digits,
    \item a fixed number of fractional digits,
    \item explicit sign and special-value channels (e.g.\ NaN, $\pm\infty$).
\end{itemize}

Configurations follow names such as \repname{digits_b10_f0} or
\repname{digits_b10_f2}, with the following convention:
\begin{itemize}[noitemsep,topsep=2pt]
    \item \texttt{bX}: use base $X$ for the digit decomposition,
    \item \texttt{fK}: use $K$ fractional digits in that base
          (e.g.\ \repname{f0} = no fractional digits, \repname{f2} = two fractional digits).
\end{itemize}
This matches the \repname{NumericDigitCategoryField}-style representations used in
our tabular models.


\section{Experimental Protocol}

\subsection{Model}

For each (task, representation) pair:
\begin{itemize}[noitemsep,topsep=2pt]
    \item Inputs are encoded with the chosen numeric representation.
    \item Encoded tokens are projected into a fixed-dimensional vector
          (the ``representation layer'').
    \item A small MLP head maps this to the task output.
\end{itemize}

Architecture and capacity are held fixed across all runs, so differences in
performance are attributable to the encodings.

\subsection{Training Setup}

\begin{itemize}[noitemsep,topsep=2pt]
    \item Shared dataset sizes, batch size, optimizer (AdamW), and epochs.
    \item Same hyperparameters across all configurations.
    \item Multiple random seeds per configuration; metrics are averaged.
    \item Primary metric:
        \begin{itemize}[noitemsep]
            \item regression tasks: R$^2$ on the test split,
            \item classification tasks: accuracy.
        \end{itemize}
\end{itemize}

Only successfully completed runs are included in the aggregates.

\section{Results}

\subsection{Global Comparison}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.98\linewidth]{matrix_summary.png}
    \caption{
        Average primary metric across tasks and representations.
        Rows are tasks; columns are representations.
        Each cell shows the mean over seeds when training on \texttt{\TrainRegime}.
    }
    \label{fig:matrix-summary}
\end{figure}

\subsection{Per-Task Summaries}

For each task, we summarize performance by representation.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{add_per_rep.png}
    \caption{Addition: primary metric per representation.}
    \label{fig:add-per-rep}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{mul_per_rep.png}
    \caption{Multiplication: primary metric per representation.}
    \label{fig:mul-per-rep}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{sin_per_rep.png}
    \caption{Sine: primary metric per representation.}
    \label{fig:sin-per-rep}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{gt_per_rep.png}
    \caption{Comparison ($x>y$): accuracy per representation.}
    \label{fig:gt-per-rep}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{parity_per_rep.png}
    \caption{Integer parity: accuracy per representation.}
    \label{fig:parity-per-rep}
\end{figure}

\subsection{Scalar vs Digit Encodings}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{scalar_vs_digits_overall.png}
    \caption{
        Overall comparison between scalar and digit encoding families.
        Bars show mean primary metric across tasks for each family.
    }
    \label{fig:scalar-vs-digits-summary}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{scalar_vs_digits_by_task.png}
    \caption{
        Per-task comparison of scalar vs digit encodings.
        Values are normalized within each task to highlight relative preference.
    }
    \label{fig:scalar-vs-digits-by-task}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{scalar_vs_digits_by_regime.png}
    \caption{
        Regime-wise comparison between scalar and digit encodings.
        Each bar averages the primary metric across tasks for a given regime.
    }
    \label{fig:scalar-vs-digits-regimes}
\end{figure}

\subsection{Regime Generalization Views}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{per_rep_regime_heatmap.png}
    \caption{
        Representation $\times$ regime heatmap.
        Each row is a representation; each column is an evaluation regime.
    }
    \label{fig:per-rep-regime-heatmap}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{per_rep_regime_lines.png}
    \caption{
        Regime robustness for selected representations.
        Each line is a representation's primary metric across regimes.
    }
    \label{fig:per-rep-regime-lines}
\end{figure}

\section{Summary Tables}

\subsection{Primary Metrics by Task and Representation}

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{l l l c}
        \toprule
        Task & Representation & Metric & Value \\
        \midrule
        % Fill rows programmatically, e.g.:
        % add   & \repname{scalar_standardize} & R$^2$    & 0.9990 \\
        % add   & \repname{digits_b10_f0}      & R$^2$    & 0.9907 \\
        % gt    & \repname{digits_b10_f0}      & accuracy & 0.9850 \\
        \bottomrule
    \end{tabular}
    \caption{
        Primary test metric per (task, representation) when trained on \texttt{\TrainRegime}.
        Use \texttt{\textbackslash repname\{...\}} for identifiers with underscores.
    }
    \label{tab:primary-metric}
\end{table}

\subsection{Family-Level Aggregates}

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{l c c}
        \toprule
        Family & Mean primary metric & Std across tasks \\
        \midrule
        % scalar & 0.9980 & 0.0015 \\
        % digits & 0.9920 & 0.0030 \\
        \bottomrule
    \end{tabular}
    \caption{
        Aggregated comparison of scalar vs digit encoding families across tasks.
    }
    \label{tab:family-aggregates}
\end{table}

\section{Implementation Notes}

\subsection{Code and Artifacts}

\begin{itemize}[noitemsep,topsep=2pt]
    \item Benchmark scripts: \texttt{scripts/numeric\_rep\_bench/}
    \item Runner: \texttt{run\_numeric\_rep\_bench.py}
    \item Status + metrics: \texttt{status.json}, per-run logs and samples
    \item Plots (this report): \texttt{runs/numeric\_rep\_bench/\RunID/plots/}
\end{itemize}

This report is intended to be generated once all configured experiments for
\RunID{} have finished; all tables and figures should be derived directly from
the recorded metrics for that run.

\subsection{Reproducibility}

\begin{itemize}[noitemsep,topsep=2pt]
    \item Fixed or logged random seeds per configuration
    \item Shared optimizer, epochs, and model capacity across encodings
    \item Consistent data generation per regime and per task
\end{itemize}

\section{Context}

The benchmark is motivated by numeric representation choices in tabular,
relational, and autoencoding models. The design is intentionally generic so the
results can inform representation choices in a wide range of architectures and
datasets.

\end{document}
